{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3b405c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:26.567461Z",
     "iopub.status.busy": "2024-08-22T07:46:26.566881Z",
     "iopub.status.idle": "2024-08-22T07:46:27.574055Z",
     "shell.execute_reply": "2024-08-22T07:46:27.572780Z"
    },
    "papermill": {
     "duration": 1.016544,
     "end_time": "2024-08-22T07:46:27.577144",
     "exception": false,
     "start_time": "2024-08-22T07:46:26.560600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43fbc9",
   "metadata": {
    "papermill": {
     "duration": 0.003426,
     "end_time": "2024-08-22T07:46:27.584775",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.581349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FIT5226 Project Stage 1 - Tabular Q-Learning for Single Agent\n",
    "\n",
    "This notebook implements a tabular Q-learning algorithm to train an agent to complete a transport task in a grid world environment. The agent's goal is to pick up an item at location A and deliver it to location B with the fewest steps possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29969d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:27.594106Z",
     "iopub.status.busy": "2024-08-22T07:46:27.593477Z",
     "iopub.status.idle": "2024-08-22T07:46:27.613279Z",
     "shell.execute_reply": "2024-08-22T07:46:27.612039Z"
    },
    "papermill": {
     "duration": 0.027869,
     "end_time": "2024-08-22T07:46:27.616221",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.588352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5, start=None, item_location=None, goal_location=None):\n",
    "        # Initialization of the environment\n",
    "        self.size = size\n",
    "        self.start = start if start else (0, 0)\n",
    "        self.item_location = item_location if item_location else (size//2, size//2)\n",
    "        self.goal_location = goal_location if goal_location else (size-1, size-1)\n",
    "        self.agent_position = self.start\n",
    "        self.has_item = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.agent_position = self.start\n",
    "        self.has_item = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns the current state of the environment.\"\"\"\n",
    "        return (self.agent_position, self.item_location, self.has_item)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes an action and updates the environment.\"\"\"\n",
    "        # Code for agent movement, reward calculation, and state update\n",
    "        if action == 'north' and self.agent_position[0] > 0:\n",
    "            self.agent_position = (self.agent_position[0] - 1, self.agent_position[1])\n",
    "        elif action == 'south' and self.agent_position[0] < self.size - 1:\n",
    "            self.agent_position = (self.agent_position[0] + 1, self.agent_position[1])\n",
    "        elif action == 'east' and self.agent_position[1] < self.size - 1:\n",
    "            self.agent_position = (self.agent_position[0], self.agent_position[1] + 1)\n",
    "        elif action == 'west' and self.agent_position[1] > 0:\n",
    "            self.agent_position = (self.agent_position[0], self.agent_position[1] - 1)\n",
    "\n",
    "        if self.agent_position == self.item_location:\n",
    "            self.has_item = True\n",
    "\n",
    "        done = False\n",
    "        if self.agent_position == self.goal_location and self.has_item:\n",
    "            reward = 50\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        if not done and self.agent_position == self.item_location and not self.has_item:\n",
    "            reward = 10\n",
    "\n",
    "        next_state = self.get_state()\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80462ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:27.625364Z",
     "iopub.status.busy": "2024-08-22T07:46:27.624929Z",
     "iopub.status.idle": "2024-08-22T07:46:27.638650Z",
     "shell.execute_reply": "2024-08-22T07:46:27.637541Z"
    },
    "papermill": {
     "duration": 0.021314,
     "end_time": "2024-08-22T07:46:27.641269",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.619955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1, grid_size=5):\n",
    "        \"\"\"Initializes the Q-learning agent.\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "        self.grid_size = grid_size\n",
    "        self.actions = ['north', 'south', 'east', 'west']\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Returns the Q-value for a given state-action pair.\"\"\"\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        \"\"\"Sets the Q-value for a given state-action pair.\"\"\"\n",
    "        self.q_table[(state, action)] = value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Chooses an action based on the epsilon-greedy policy.\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, action) for action in self.actions]\n",
    "            max_q = max(q_values)\n",
    "            return self.actions[q_values.index(max_q)]\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"Updates the Q-value based on the received reward and next state.\"\"\"\n",
    "        next_q_values = [self.get_q_value(next_state, a) for a in self.actions]\n",
    "        best_next_q = max(next_q_values)\n",
    "        old_q_value = self.get_q_value(state, action)\n",
    "        new_q_value = old_q_value + self.alpha * (reward + self.gamma * best_next_q - old_q_value)\n",
    "        self.set_q_value(state, action, new_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050a731a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:27.650504Z",
     "iopub.status.busy": "2024-08-22T07:46:27.650087Z",
     "iopub.status.idle": "2024-08-22T07:46:27.658170Z",
     "shell.execute_reply": "2024-08-22T07:46:27.656905Z"
    },
    "papermill": {
     "duration": 0.016158,
     "end_time": "2024-08-22T07:46:27.661057",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.644899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_agent(agent, environment, episodes=1000):\n",
    "    \"\"\"Trains the Q-learning agent.\"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb249a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:27.670247Z",
     "iopub.status.busy": "2024-08-22T07:46:27.669813Z",
     "iopub.status.idle": "2024-08-22T07:46:27.678016Z",
     "shell.execute_reply": "2024-08-22T07:46:27.676727Z"
    },
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-08-22T07:46:27.680768",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.664804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_agent(agent, environment, trials=100):\n",
    "    \"\"\"Tests the Q-learning agent after training.\"\"\"\n",
    "    total_steps = 0\n",
    "    for trial in range(trials):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        total_steps += steps\n",
    "    average_steps = total_steps / trials\n",
    "    print(f\"Average steps taken: {average_steps}\")\n",
    "    return average_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9457e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T07:46:27.690155Z",
     "iopub.status.busy": "2024-08-22T07:46:27.689628Z",
     "iopub.status.idle": "2024-08-22T07:46:27.699371Z",
     "shell.execute_reply": "2024-08-22T07:46:27.697952Z"
    },
    "papermill": {
     "duration": 0.017684,
     "end_time": "2024-08-22T07:46:27.702177",
     "exception": false,
     "start_time": "2024-08-22T07:46:27.684493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_agent(environment, agent, episodes=10):\n",
    "    \"\"\"Visualizes the agent's path in the grid world.\"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        path = [state[0]]\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = environment.step(action)\n",
    "            path.append(next_state[0])\n",
    "            state = next_state\n",
    "        \n",
    "        grid = np.zeros((environment.size, environment.size))\n",
    "        for position in path:\n",
    "            grid[position] += 1\n",
    "        plt.imshow(grid, cmap='Blues', origin='upper')\n",
    "        plt.colorbar(label='Number of visits')\n",
    "        plt.title(f'Path in episode {episode + 1}')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.537777,
   "end_time": "2024-08-22T07:46:28.228788",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-22T07:46:22.691011",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
